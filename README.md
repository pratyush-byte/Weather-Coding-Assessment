# Weather Coding Challenge â€“ QuickÂ Guide

## ðŸ› Â 1Â Â·Â Setâ€‘up (oneâ€‘time)

1. **Create & activate** a virtualâ€‘environment
   python3 -m venv venv && source venv/bin/activate  # Windows: venv\Scripts\activate
2. **Install libraries** (all problems)
   Initial_checks_install.py

## My solution and approach including with step by step instructions to run the code
## ProblemÂ 1Â â€“ Schema

3. **Create the empty tables**
## Due to storage issue in GIT i have deleted the weather.db file, so please run this file
```bash
   python create_db.py       # produces weather.db with 3 empty tables
```                   


| Table                  | What it stores                   | PrimaryÂ Key          |
| ---------------------- | -------------------------------- | -------------------- |
| `station`              | Every weatherâ€‘station ID         | `id`                 |
| `weather_daily`        | *stationÂ Ã—Â day* raw observations | `(station_id, date)` |
| `weather_yearly_stats` | *stationÂ Ã—Â year* aggregates      | `(station_id, year)` |

*Integers keep raw units:â€¯tenthâ€‘Â°C & tenthâ€‘mm; `â€‘9999` â†’Â `NULL`.*

---

## ProblemÂ 2Â â€“ Ingestion

### âœ±â€¯Significance

Turn thousands of flatâ€‘files into relational rows **safely** (no duplicates) and **quickly** (bulk inserts, batching).

### ðŸš€Â Key Features

| Aspect                | Detail                                                                  |
| --------------------- | ----------------------------------------------------------------------- |
| **Script**            | `ingest.py`                                                             |
| **Idempotent**        | `ON CONFLICT DO NOTHING` (composite PK) â€” reruns never duplicate rows.  |
| **Duplicate Metrics** | Every batch logs *new* vs. *dup*; perâ€‘file totals in `logs/ingest.log`. |
| **Batching**          | SQLite â‰¤â€¯180 rows/flush (under 999â€‘param limit).                        |
| **Logging**           | Console **and** file `logs/ingest.log`.                                 |

### â–¶ï¸ŽÂ Run

```bash
python ingest.py      
```

Sample output

```
USC00110072 â€¦  10â€¯957 new Â· 0 dup
USC00110073 â€¦       0 new Â· 10â€¯957 dup
Done: 10â€¯957â€¯890 new Â· 10â€¯957 dup
```

---

## ProblemÂ 3Â â€“ DataÂ Analysis

### âœ±â€¯Significance

Condense millions of daily rows into **one row per stationâ€¯Ã—â€¯year**, ready for fast API queries.  A diff log detects data drift instantly.

### ðŸš€Â KeyÂ Features

| Aspect          | Detail                                                 |
| --------------- | ------------------------------------------------------ |
| **Script**      | `Data_analysis.py`                                 |
| **Model**       | Table `weather_yearly_stats` (PK `(station_id, year)`) |
| **SQLiteâ€‘only** | One aggregate `GROUPÂ BY` query â€” no temp tables.       |
| **Idempotent**  | Table is rebuilt every run.                            |
| **DiffÂ Log**    | NEW / CHG rows written to `logs/data_analysis.log`.    |

### â–¶ï¸ŽÂ Run

```bash
python Data_analysis.py
```

Example tail of log

```
NEW   USC00110072 1985  Tmax=12.3  Tmin=-1.5  Precip=67.4
â–² Yearly aggregation finished: 4â€¯820 added Â· 0 changed Â· 0 removed Â· 0 unchanged (4.5Â s)
```

---

## ProblemÂ 4Â â€“ RESTÂ API

### âœ±â€¯Significance

Serve cleaned weather data to any client via JSON with **filtering, pagination & live docs**.

### ðŸš€Â KeyÂ Features

| Aspect             | Detail                                                                      |
| ------------------ | --------------------------------------------------------------------------- |
| **App**            | `app_flask.py` (FlaskÂ +Â Flaskâ€‘RESTX)                                        |
| **Endpoints**      | `/api/weather` Â· `/api/weather/stats`                                       |
| **Filters**        | `station_id`, `date` (range) on daily; `station_id`, `year` on yearly stats |
| **Pagination**     | `page` & `page_size` query params; 404 if pageâ€‘outâ€‘ofâ€‘range                 |
| **Docs (Swagger)** | OpenAPI UI at `/docs` (autoâ€‘generated)                                      |

### â–¶ï¸ŽÂ Run

```bash
export FLASK_APP=app_flask.py     # Windows: set FLASK_APP=app_flask.py
flask run                          # starts http://127.0.0.1:5000/
# OR RUN
python app_flask.py
```

*Open* [http://127.0.0.1:5000/docs](http://127.0.0.1:5000/docs) for Swagger, or test with Postman:

```
GET http://127.0.0.1:5000/api/weather?page=1&page_size=5&station_id=USC00110072
```

---

## ðŸ”ŽÂ Verify Everything

```
python check_counts.py            # rows & samples from all three tables
pytest -q                         # runs unit tests (ingestion only)
```

## EXTRA CREDIT  Deployment

I donâ€™t have hands-on production cloud experience yet, but from my studies and research this is the approach I would take. Iâ€™d first containerize the whole project with Docker, ensuring the code and dependencies run identically everywhere. Iâ€™d push that image to AWS Elastic Beanstalk, which can launch and manage an EC2 instance, pull the image, and keep the Flask API live behind an autoscaling load balancer. For storage Iâ€™d provision a managed Amazon RDS PostgreSQL database during the same Beanstalk setup, then drop the generated connection string into my environment variables. Finally, Iâ€™d automate the nightly ingest.py run using Elastic Beanstalkâ€™s built-in cron.yaml (or a lightweight AWS Lambda trigger if more flexibility is needed). This stack gives me a fully managed deploymentâ€”API, database, and scheduled ingestionâ€”while letting me focus on code rather than server maintenance.
